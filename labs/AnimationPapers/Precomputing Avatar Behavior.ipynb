{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2121be1-37c6-4a75-88e1-b56542cf7f1c",
   "metadata": {},
   "source": [
    "# Precomputing Avatar Behavior From Human Motion Data\n",
    "\n",
    "by Jehee Lee and Kang Hoon Lee\n",
    "Siggraph 2004\n",
    "\n",
    "Notebook by Jerome Eippers, 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676f2d42-1766-4b54-833d-1840893d4c8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "from dataclasses import dataclass, field\n",
    "import numpy as np\n",
    "from ipywidgets import widgets, interact\n",
    "from random import randrange\n",
    "import pickle\n",
    "\n",
    "import ipyanimlab as lab\n",
    "\n",
    "viewer = lab.Viewer(move_speed=5, width=1280, height=720)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93687c1a-a9b6-4319-a178-d7a57550d7e6",
   "metadata": {},
   "source": [
    "## Motion Graph\n",
    "\n",
    "### Load Motion Graph Data\n",
    "\n",
    "We will load the motion graph data from the Motion Graph notebook.  \n",
    "In this we will have the animation, what frames are part of the animation and the local minima used used to create the transitions.\n",
    "\n",
    "We will also load the character and add the extra foot contact bones, so we can quickly compute a simple foot locking mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ea6539-d66b-47c2-94c3-9f2a3f876741",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load the character\n",
    "\n",
    "character = viewer.import_usd_asset('AnimLabSimpleMale.usd')\n",
    "character.add_bone('LeftHeel', np.array([1,0,0,0]), np.array([9.2,0,-12]), 'LeftFoot')\n",
    "character.add_bone('LeftBall', np.array([1,0,0,0]), np.array([14.5,0,8.22]), 'LeftFoot')\n",
    "character.add_bone('RightHeel', np.array([1,0,0,0]), np.array([-9.2,0,-12]), 'RightFoot')\n",
    "character.add_bone('RightBall', np.array([1,0,0,0]), np.array([-14.5,0,8.22]), 'RightFoot')\n",
    "\n",
    "left_heel = character.bone_index('LeftHeel')\n",
    "left_ball = character.bone_index('LeftBall')\n",
    "right_heel = character.bone_index('RightHeel')\n",
    "right_ball = character.bone_index('RightBall')\n",
    "left_foot = character.bone_index('LeftFoot')\n",
    "right_foot = character.bone_index('RightFoot')\n",
    "foottag_indices = np.asarray([left_heel, left_ball, right_heel, right_ball], dtype=np.int8)\n",
    "print(foottag_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c814bcdc-afac-47e8-90cc-31055ff80caf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a sphere for debugging purpose\n",
    "\n",
    "sphere = viewer.create_sphere(radius=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d435f6-2b42-4731-aa6c-6cc8ae7dc97b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load the pre computed motiongraph data\n",
    "\n",
    "with open('motion_graph_walking_rawdata.dat', 'rb') as f:\n",
    "    (animation, window_size, animation_frame_validity, animation_local_minima) = pickle.load(f)\n",
    "    \n",
    "animmap = lab.AnimMapper(character)\n",
    "animation = animmap(animation)\n",
    "\n",
    "frame_count = animation.quats.shape[0]\n",
    "bone_count = character.bone_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b4d0a0-f70b-4d47-a35a-b3adc2ab2d13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# compute the foot contacts\n",
    "\n",
    "gquats, gpos = lab.utils.quat_fk(animation.quats, animation.pos, animation.parents)\n",
    "\n",
    "foot_tags = np.zeros([frame_count, 4], dtype=np.bool_)\n",
    "foot_tags[:, :2], foot_tags[:, 2:] = lab.utils.extract_feet_contacts(gpos, [left_heel, left_ball], [right_heel, right_ball],  0.04)\n",
    "\n",
    "# smooth out a little the signal ( we keep the signal on if it switch off for one or 2 frames )\n",
    "for frame in range(2, frame_count-2):\n",
    "    for c in range(4):\n",
    "        foot_tags[frame, c] = foot_tags[frame, c] or (foot_tags[frame-2, c] and foot_tags[frame+2, c])\n",
    "    \n",
    "for frame in range(1, frame_count-1):\n",
    "    for c in range(4):\n",
    "        foot_tags[frame, c] = foot_tags[frame, c] or (foot_tags[frame-1, c] and foot_tags[frame+1, c])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06eff34-2521-4c14-9f2a-cd0e7f15350b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def render(frame):\n",
    "    \n",
    "    anim = animation\n",
    "    p = (anim.pos[frame,...])\n",
    "    q = (anim.quats[frame,...])\n",
    "        \n",
    "    a = lab.utils.quat_to_mat(q, p)\n",
    "    viewer.set_shadow_poi(p[0])\n",
    "    \n",
    "    viewer.begin_shadow()\n",
    "    viewer.draw(character, a)\n",
    "    viewer.end_shadow()\n",
    "    \n",
    "    viewer.begin_display()\n",
    "    viewer.draw_ground()\n",
    "    viewer.draw(character, a)\n",
    "    viewer.end_display()\n",
    "\n",
    "    viewer.disable(depth_test=True)\n",
    "   \n",
    "    viewer.draw_axis(character.world_skeleton_xforms(a), 5)\n",
    "    viewer.draw_lines(character.world_skeleton_lines(a))\n",
    "    \n",
    "    viewer.execute_commands()\n",
    "    \n",
    "interact(\n",
    "    render, \n",
    "    frame=lab.Timeline(max=frame_count-1)\n",
    ")\n",
    "viewer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71430481-3dfc-4470-8c8f-f92e54eaf8d4",
   "metadata": {},
   "source": [
    "### Create the Motion Graph States\n",
    "\n",
    "This time instead of list of states (with several frames) and edges (connecting frames),  \n",
    "we will have :\n",
    "\n",
    "* a list of states : a single frame where a choice can be made.\n",
    "* for each state a list of actions : where each action is list of frames to play to reach the next state. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a7be6e-1529-4efa-a715-5b8b2af663f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class State:\n",
    "    \"\"\"A state in the graph.\"\"\"\n",
    "    frame: int = 0\n",
    "    actions: list[int] = field(default_factory=list)\n",
    "    reversed_actions: list[int] = field(default_factory=list)\n",
    "    \n",
    "@dataclass\n",
    "class Action:\n",
    "    \"\"\"A directed action.\"\"\"\n",
    "    start: int = 0\n",
    "    end: int = 0\n",
    "    blend: bool = False\n",
    "    is_valid_action: bool = False\n",
    "    next_state_id: int = 0\n",
    "    \n",
    "# prepare the states data\n",
    "states = []\n",
    "for i in range(frame_count):\n",
    "    states.append(State(frame=i))\n",
    "    if animation_frame_validity[i]:\n",
    "        states[i].actions.append(Action(start=i, end=i+1))\n",
    "        \n",
    "# add the transitions\n",
    "horizontal, vertical = np.where(animation_local_minima == 1)\n",
    "for i, j in zip(horizontal, vertical):\n",
    "    if animation_frame_validity[i] and animation_frame_validity[j]:\n",
    "        states[i].actions.append(Action(start=j-window_size, end=j, blend=True))\n",
    "\n",
    "# starting from an action we check if the targetted state has only one action\n",
    "# in that case we extend the action to target directly the next next state, ... and so on\n",
    "def collapse_action(action):\n",
    "    end = action.end\n",
    "    while len(states[end].actions) == 1:\n",
    "        end = states[end].actions[0].end\n",
    "    if len(states[end].actions) > 1:\n",
    "        action.end = end\n",
    "        action.is_valid_action = True\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# collapse actions\n",
    "stack = next((list(state.actions) for state in states if len(state.actions) > 0 ))\n",
    "while stack:\n",
    "    action = stack.pop()\n",
    "    if (action.is_valid_action == False and collapse_action(action)):\n",
    "        stack += states[action.end].actions\n",
    "\n",
    "# compute reverse actions\n",
    "for state in states:\n",
    "    for action in state.actions:\n",
    "        states[action.end].reversed_actions.append(action)\n",
    "        \n",
    "# delete actions that are not valid, and all the states that are unreachable\n",
    "for state in reversed(states):\n",
    "    for action in reversed(state.actions):\n",
    "        if action.is_valid_action == False:\n",
    "            state.actions.remove(action)\n",
    "    if not state.actions:\n",
    "        states.remove(state)\n",
    "\n",
    "# collapse all states that have only one action left that is not a transition\n",
    "for state in reversed(states):\n",
    "    if len(state.actions) == 1 and state.actions[0].start == state.frame:\n",
    "        for action in state.reversed_actions:\n",
    "            action.end = state.actions[0].end\n",
    "        states.remove(state)\n",
    "        \n",
    "# clear the reverse\n",
    "for state in states:\n",
    "    state.reversed_actions.clear()\n",
    "    \n",
    "def find_state_index(frame):\n",
    "    for state in states:\n",
    "        if state.frame == frame:\n",
    "            return states.index(state)\n",
    "    return None\n",
    "\n",
    "for state in states:\n",
    "    for action in state.actions:\n",
    "        action.next_state_id = find_state_index(action.end)\n",
    "        \n",
    "state_count = len(states)\n",
    "        \n",
    "# sort the states by frame\n",
    "# (only for debug)\n",
    "states = sorted(states, key=lambda n : n.frame)\n",
    "\n",
    "display(state_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9846a7f-6849-459c-9f4e-627b5f5bdccc",
   "metadata": {},
   "source": [
    "### Precompute root motion\n",
    "\n",
    "For each action we can pre compute he root motion relative to the first frame.  \n",
    "This will be usefull when playing the animation and also for the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6084feb8-cfd8-4155-8596-f673fa1fe585",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# compute trajectories of displacement for each actions\n",
    "gquats, gpos = lab.utils.quat_fk(animation.quats, animation.pos, animation.parents)\n",
    "\n",
    "for state in states:\n",
    "    for action in state.actions:\n",
    "        action.start = int(action.start)\n",
    "        action.end = int(action.end)\n",
    "        blend_frame_count = action.blend * window_size\n",
    "        start_frame, end_frame = action.start+1, action.end+1\n",
    "        \n",
    "        if end_frame-start_frame <= 0:\n",
    "            raise Exception(f\"state {states.index(state)} action {action}\")\n",
    "                \n",
    "        # bring anim relative to first frame\n",
    "        boq, bop = animation.quats[start_frame-1,0,:], animation.pos[start_frame-1,0,:]\n",
    "        boqi, bopi = lab.utils.qp_inv((boq, bop))\n",
    "        bq, bp = animation.quats[start_frame:end_frame,0,:].copy(), animation.pos[start_frame:end_frame,0,:].copy()\n",
    "        bq, bp = lab.utils.qp_mul((boqi[np.newaxis,...], bopi[np.newaxis,...]),(bq, bp))\n",
    "    \n",
    "        action.trajectory_quats = bq\n",
    "        action.trajectory_pos = bp\n",
    "        \n",
    "        action.local_trajectory_pos = action.trajectory_pos.copy()\n",
    "        action.local_trajectory_quats = action.trajectory_quats.copy()\n",
    "        \n",
    "        action.local_trajectory_quats[1:], action.local_trajectory_pos[1:] = lab.utils.qp_mul(\n",
    "            lab.utils.qp_inv((action.local_trajectory_quats[:-1], action.local_trajectory_pos[:-1])),\n",
    "            (action.local_trajectory_quats[1:], action.local_trajectory_pos[1:])\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ab980a-3814-4f8d-81de-a773dc9bf047",
   "metadata": {},
   "source": [
    "### Animation Player\n",
    "\n",
    "A simple player that can take an animation and plays frame after frame everytime we tick it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1430e709-62ef-4f3d-a146-d1e49c34f6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FootLock:\n",
    "    def __init__(self, toe_id, ankle_boneid):\n",
    "        self.toe_id = toe_id\n",
    "        self.ankle_boneid = ankle_boneid\n",
    "        self.ankle_pos = np.zeros([3], dtype=np.float32)\n",
    "        self.ankle_quat = np.array([1,0,0,0], dtype=np.float32)\n",
    "        self.toe_lock = False\n",
    "        self.toe_position = np.zeros([3], dtype=np.float32)\n",
    "        self.toe_error = np.zeros([3], dtype=np.float32)\n",
    "        self.blend = 0\n",
    "        \n",
    "    def compute(self, positions, quaternions, frame):\n",
    "        gq, gp = lab.utils.quat_fk(quaternions, positions, animation.parents)\n",
    "        \n",
    "        if foot_tags[frame, self.toe_id] :\n",
    "            if self.toe_lock == False:\n",
    "                self.toe_lock = True\n",
    "                self.toe_position = gp[foottag_indices[self.toe_id]]\n",
    "                self.toe_position[1] = 0\n",
    "                self.blend = 0\n",
    "            \n",
    "            self.toe_error = (self.toe_position - gp[foottag_indices[self.toe_id]])\n",
    "            t = 1.0\n",
    "            if self.blend < 3:\n",
    "                t = float(self.blend) / 4.0\n",
    "                self.ankle_pos += self.toe_error * t\n",
    "                self.blend += 1\n",
    "            self.ankle_pos = gp[self.ankle_boneid] + self.toe_error * t\n",
    "            self.ankle_quat = gq[self.ankle_boneid]\n",
    "            \n",
    "        else:\n",
    "            if self.toe_lock:\n",
    "                self.blend = 5\n",
    "            self.toe_lock = False\n",
    "            self.ankle_pos = gp[self.ankle_boneid]\n",
    "            self.ankle_quat = gq[self.ankle_boneid]\n",
    "            \n",
    "            if self.blend > 0:\n",
    "                t = float(self.blend) / 6.0\n",
    "                self.ankle_pos += self.toe_error * t\n",
    "                self.blend -= 1\n",
    "            \n",
    "\n",
    "class AnimPlayer:\n",
    "    def __init__(self, start_state):\n",
    "        self.current_action = None\n",
    "        self.current_action_framei = 1\n",
    "        self.positions = np.zeros([bone_count, 3], dtype=np.float32)\n",
    "        self.quaternions = np.array([1,0,0,0], dtype=np.float32)[np.newaxis,...].repeat(bone_count, axis=0)\n",
    "        self.last_played_frame = states[start_state].frame\n",
    "        self.transition = None\n",
    "        self.next_state_id = start_state\n",
    "        self.left_foot = FootLock(1, left_foot)\n",
    "        self.right_foot = FootLock(3, right_foot)\n",
    "        \n",
    "    def tick(self):\n",
    "        if self.current_action is not None:\n",
    "            last_root_q = self.quaternions[0].copy()\n",
    "            last_root_p = self.positions[0].copy()\n",
    "            \n",
    "            self.last_played_frame = self.current_action.start + self.current_action_framei\n",
    "            self.positions = animation.pos[self.last_played_frame, :, :].copy()\n",
    "            self.quaternions = animation.quats[self.last_played_frame, :, :].copy()\n",
    "            \n",
    "            \n",
    "            if self.transition is not None:\n",
    "                from_frame, from_frame_i = self.transition\n",
    "                from_frame_i += 1\n",
    "                from_frame += 1\n",
    "                if from_frame_i <= window_size:\n",
    "                    t = float(from_frame_i)/float(window_size+1)\n",
    "                    t = -2.0*t**3 + 3*t**2\n",
    "                    \n",
    "                    self.positions = (t) * self.positions + (1.0-t) * animation.pos[from_frame, :, :]\n",
    "                    self.quaternions = lab.utils.quat_slerp(animation.quats[from_frame, :, :], self.quaternions, t)\n",
    "                    \n",
    "                    self.transition = (from_frame, from_frame_i)\n",
    "                else:\n",
    "                    self.transition = None\n",
    "                    \n",
    "            \n",
    "            self.quaternions[0], self.positions[0] = lab.utils.qp_mul(\n",
    "                (last_root_q, last_root_p),\n",
    "                (self.current_action.local_trajectory_quats[self.current_action_framei-1], self.current_action.local_trajectory_pos[self.current_action_framei-1])\n",
    "            )\n",
    "            \n",
    "            self.current_action_framei += 1\n",
    "            if self.current_action.start + self.current_action_framei > self.current_action.end :\n",
    "                self.current_action = None\n",
    "                self.current_action_framei = 0\n",
    "                \n",
    "            self.left_foot.compute(self.positions, self.quaternions, self.last_played_frame)\n",
    "            self.right_foot.compute(self.positions, self.quaternions, self.last_played_frame)\n",
    "\n",
    "            self.quaternions, self.positions = lab.utils.limb_ik(\n",
    "                self.quaternions[np.newaxis, ...], self.positions[np.newaxis, ...], animation.parents, animation.bones,\n",
    "                np.array([self.left_foot.ankle_quat, self.right_foot.ankle_quat])[np.newaxis, ...],\n",
    "                np.array([self.left_foot.ankle_pos, self.right_foot.ankle_pos])[np.newaxis, ...],\n",
    "            )\n",
    "            self.quaternions, self.positions = self.quaternions[0], self.positions[0]\n",
    "                \n",
    "                \n",
    "        return self.current_action != None\n",
    "    \n",
    "    def set_action(self, action):\n",
    "        self.current_action = action\n",
    "        self.next_state_id = action.next_state_id\n",
    "        self.current_action_framei = 1\n",
    "        self.transition = None\n",
    "        if action is not None and action.blend:\n",
    "            self.transition = (self.last_played_frame, 0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c42534-b2ad-4e05-9db8-235920c74720",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "player = AnimPlayer(0)\n",
    "\n",
    "def render(frame):\n",
    "\n",
    "    if player.current_action is None:\n",
    "        state = states[player.next_state_id]\n",
    "        e = randrange(0, len(state.actions))\n",
    "        action = state.actions[e]\n",
    "        player.set_action(action)\n",
    "                \n",
    "    player.tick()\n",
    "    display((player.last_played_frame, player.transition))\n",
    "    \n",
    "    q = player.quaternions\n",
    "    p = player.positions\n",
    " \n",
    "    a = lab.utils.quat_to_mat(q, p)\n",
    "    viewer.set_shadow_poi(p[0])\n",
    "    \n",
    "    viewer.begin_shadow()\n",
    "    viewer.draw(character, a)\n",
    "    viewer.end_shadow()\n",
    "    \n",
    "    viewer.begin_display()\n",
    "    viewer.draw_ground()\n",
    "    viewer.draw(character, a)\n",
    "    viewer.end_display()\n",
    "\n",
    "    viewer.disable(depth_test=True)\n",
    "    \n",
    "    viewer.execute_commands()\n",
    "    \n",
    "interact(\n",
    "    render, \n",
    "    frame=lab.Timeline(max=100)\n",
    ")\n",
    "viewer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0db6ab8-3def-4d7a-93b8-25dc1c6ec271",
   "metadata": {},
   "source": [
    "## Control Policy\n",
    "\n",
    "### The reward function $R(s, e, a)$ \n",
    "\n",
    "is given by:\n",
    "\n",
    "$$\n",
    "R(s, e, a) = \\max_t \\left( \\gamma^t \\exp\\left(-\\frac{\\|p(t) - p_d\\|}{10}\\right) \\right),\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $s$: The current state of the avatar.\n",
    "* $e$: The target's state.\n",
    "* $a$: The action taken by the avatar.\n",
    "* $p(t)$: The avatar's position trajectory during the action $a$.\n",
    "* $p_d$: The target's position in the polar grid.\n",
    "* $\\gamma \\in (0, 1)$: The discount factor, which penalizes delayed rewards.\n",
    "* $|\\cdot|$: The Euclidean distance between $p(t)$ and $p_d$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2763c11f-4ace-4cba-8565-b3a4ae0bb1b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_action_count = 0\n",
    "max_action_length = 0\n",
    "for state in states:\n",
    "    max_action_count = max(max_action_count, len(state.actions))\n",
    "    for action in state.actions:\n",
    "        max_action_length = max(max_action_length, action.end -  action.start)\n",
    "display(max_action_count)\n",
    "display(max_action_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d50f5f-e060-4037-a08a-6eee968578c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "target_positions = [[[0,0,0]]]\n",
    "for i in range(6):\n",
    "    target_positions.append (np.array([\n",
    "        np.cos(np.linspace(0, np.pi*2, 9+i)[:-1]),\n",
    "        [0]*(8+i),\n",
    "        np.sin(np.linspace(0, np.pi*2, 9+i)[:-1])\n",
    "    ]).T * (36*(i)+20) )\n",
    "    \n",
    "target_positions = np.concatenate(target_positions)\n",
    "target_positions_count = target_positions.shape[0]\n",
    "display(target_positions_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8f85d6-a97a-4402-b511-0272b60386bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_discount = .98\n",
    "time_discounts = np.array([time_discount**t for t in range(max_action_length)])\n",
    "\n",
    "immediate_rewards = np.ones([state_count, target_positions_count, max_action_count]) * -1\n",
    "next_states = np.ones([state_count, target_positions_count, max_action_count], dtype=np.int32) * -1\n",
    "\n",
    "for i in range(state_count):\n",
    "    for j in range(target_positions.shape[0]):\n",
    "        for a, action in enumerate(states[i].actions):\n",
    "            length = action.end - action.start\n",
    "            v = action.trajectory_pos - target_positions[j]\n",
    "            immediate_rewards[i,j,a] = np.max(np.exp( -np.sqrt(np.sum(v*v, axis=1)) / 10.0 ) * time_discounts[:length])\n",
    "            \n",
    "            # find the closest target_positions after we moved\n",
    "            spositions = lab.utils.quat_mul_vec(action.trajectory_quats[-1][np.newaxis,:], target_positions) + action.trajectory_pos[-1] - target_positions[j]\n",
    "            target_state_id = np.sum(spositions*spositions, axis=1).argmin()\n",
    "                        \n",
    "            next_states[i,j,a] = action.next_state_id*target_positions_count + target_state_id\n",
    "\n",
    "immediate_rewards = immediate_rewards.reshape(-1, max_action_count)\n",
    "next_states = next_states.reshape(-1, max_action_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138c841b-16be-4bea-b1f3-795e3432cf7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamepad = widgets.Controller(index=0)\n",
    "gamepad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efa3b3c-1963-44d2-9d75-9fb064f9fe99",
   "metadata": {},
   "outputs": [],
   "source": [
    "player = AnimPlayer(0)\n",
    "\n",
    "player.selected_position = 0\n",
    "player.selected_qp = ()\n",
    "\n",
    "def render(frame):              \n",
    "\n",
    "    posx = -gamepad.axes[1].value * 500\n",
    "    posz = gamepad.axes[0].value * 500\n",
    "    \n",
    "    if player.current_action is None:\n",
    "        state = states[player.next_state_id]\n",
    "        \n",
    "        spositions = lab.utils.quat_mul_vec(player.quaternions[0][np.newaxis,:], target_positions) + player.positions[0] - np.array([posx,0,posz])\n",
    "        target_state_id = np.sum(spositions*spositions, axis=1).argmin()\n",
    "        \n",
    "        action_id = immediate_rewards[player.next_state_id * target_positions_count + target_state_id, :].argmax()\n",
    "\n",
    "        player.selected_position = target_state_id\n",
    "        player.selected_qp = (player.quaternions[0], player.positions[0])\n",
    "        \n",
    "        action = state.actions[action_id]\n",
    "        player.set_action(action)\n",
    "                \n",
    "    player.tick()\n",
    "    display((player.last_played_frame, player.transition))\n",
    "    \n",
    "    q = player.quaternions\n",
    "    p = player.positions\n",
    " \n",
    "    a = lab.utils.quat_to_mat(q, p)\n",
    "    viewer.set_shadow_poi(p[0])\n",
    "    \n",
    "    viewer.begin_shadow()\n",
    "    viewer.draw(character, a)\n",
    "    viewer.end_shadow()\n",
    "    \n",
    "    viewer.begin_display()\n",
    "    viewer.draw_ground()\n",
    "    viewer.draw(character, a)\n",
    "    sphere.materials()[0].set_albedo(np.array([1,0,0], dtype=np.float32))\n",
    "    target_matrix = np.eye(4, dtype=np.float32)\n",
    "    target_matrix[0, 3] = posx\n",
    "    target_matrix[2, 3] = posz\n",
    "    viewer.draw(sphere, target_matrix)\n",
    "    viewer.end_display()\n",
    "\n",
    "    viewer.disable(depth_test=True)\n",
    "\n",
    "    states_matrices = np.eye(4, dtype=np.float32)[np.newaxis,...].repeat(target_positions.shape[0], axis=0)\n",
    "    states_matrices[:, :3, 3] = lab.utils.quat_mul_vec(player.selected_qp[0][np.newaxis,:], target_positions) + player.selected_qp[1]\n",
    "    viewer.draw_axis(states_matrices, 2)  \n",
    "    \n",
    "    viewer.draw_axis(states_matrices[player.selected_position][np.newaxis, ...], 10)\n",
    "    if player.current_action:\n",
    "        lines = lab.utils.quat_mul_vec(player.selected_qp[0][np.newaxis,:], player.current_action.trajectory_pos) + player.selected_qp[1]\n",
    "        viewer.draw_lines(lines.repeat(2, axis=0)[1:-1].astype(np.float32))\n",
    "        \n",
    "    viewer.execute_commands()\n",
    "    \n",
    "interact(\n",
    "    render, \n",
    "    frame=lab.Timeline(max=100)\n",
    ")\n",
    "viewer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86de8b6-def9-4ae8-8931-12e120cc441a",
   "metadata": {},
   "source": [
    "### Markov Decision Process (MDP)\n",
    "\n",
    "The Markov property ensures the next state depends only on the current state and action, not on the history.\n",
    "\n",
    "\n",
    "An MDP is defined by the tuple $(S, A, R, \\gamma, T)$, where:\n",
    "- $S$: The set of states $s,e$.\n",
    "- $A$: The set of actions.\n",
    "- $R(s, e, a)$: The reward function, which gives the immediate reward for taking action $a$ in state $s,e$.\n",
    "- $\\gamma \\in [0, 1]$: The discount factor, which balances the importance of future rewards.\n",
    "- $T(s, e, a) \\to s', e'$: The transition function that determines the next state $s', e'$ after taking action $a$ in state $s, e$.\n",
    "\n",
    "The goal in an MDP is to find a **policy** (a mapping from states to actions) that **maximizes the expected cumulative reward** starting from an initial state $s_0$. The cumulative reward is:\n",
    "\n",
    "$$\n",
    "G = \\sum_{t=0}^\\infty \\gamma^t R(s_t, e_t, a_t),\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $s_t, e_t$: The state at time $t$.\n",
    "- $a_t$: The action taken at time $t$.\n",
    "\n",
    "\n",
    "### Value Function\n",
    "\n",
    "The **Value Function** quantifies the cumulative reward of starting in state $(s, e)$ and following the optimal policy thereafter. It is recursively defined as:\n",
    "\n",
    "$$\n",
    "V(s, e) := \\max_a \\left[ R(s, e, a) + \\gamma^t V(s', e') \\right],\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $R(s, e, a)$: The immediate reward for taking action $a$ in state $(s, e)$.\n",
    "- $s', e'$: The resulting avatar and target states after taking action $a$.\n",
    "- $t$: The duration of the action.\n",
    "- $\\gamma^t$: Discounts future rewards, encouraging quicker actions.\n",
    "\n",
    "\n",
    "\n",
    "### Bellman Update Rule\n",
    "\n",
    "The **Bellman Update Rule** refines the value function iteratively, ensuring optimality for every state-action pair:\n",
    "\n",
    "In my case I will actually use a value function **$V(s, e, a)$**\n",
    "\n",
    "Steps:\n",
    "1. **Sample** a state-action pair $(s, e, a)$.\n",
    "2. **Compute the reward $R(s, e, a)$** for the current action.\n",
    "3. **Estimate the value** of the next state $\\max_{a'} V(s', e', a')$:\n",
    "4. **Update $V(s, e, a)$** based on the maximum cumulative reward.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7fa258-de65-4195-9ba5-3ff6679375fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "out_value_function = widgets.Output(layout={'border': '1px solid black'})\n",
    "display(out_value_function)\n",
    "\n",
    "future_reward_discount = .97\n",
    "value_function = immediate_rewards.copy()\n",
    "\n",
    "last_total_reward = 0\n",
    "for epoch in range(1000):\n",
    "    for _ in range(30000):\n",
    "        state_id = randrange(0, state_count)\n",
    "        state_space_id = state_id * target_positions_count + randrange(0, target_positions_count)\n",
    "        action_id = randrange(0, len(states[state_id].actions))\n",
    "        action = states[state_id].actions[action_id]\n",
    "\n",
    "        length = action.end - action.start\n",
    "\n",
    "        # reward\n",
    "        reward = immediate_rewards[state_space_id, action_id]\n",
    "\n",
    "        # future\n",
    "        next_state = next_states[state_space_id, action_id]\n",
    "        next_max = np.max(value_function[next_state,:])\n",
    "\n",
    "        # Update\n",
    "        value_function[state_space_id, action_id] = reward + next_max * future_reward_discount**length\n",
    "\n",
    "\n",
    "    total_reward = np.sum(value_function)\n",
    "    with out_value_function:\n",
    "        out_value_function.clear_output()\n",
    "        display(f\"epoch {epoch} :: total {total_reward}, difference with previous epoch {total_reward - last_total_reward}\")\n",
    "    if np.abs(total_reward - last_total_reward) < 1:\n",
    "        last_total_reward = total_reward\n",
    "        break\n",
    "    last_total_reward = total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5b7afc-7ad4-477c-ba92-64bd411fe949",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "player = AnimPlayer(0)\n",
    "\n",
    "player.selected_position = 0\n",
    "player.selected_qp = ()\n",
    "\n",
    "def render(frame):              \n",
    "\n",
    "    posx = -gamepad.axes[1].value * 500\n",
    "    posz = gamepad.axes[0].value * 500\n",
    "    is_immediate = gamepad.buttons[0].value > 0.2\n",
    "    \n",
    "    if player.current_action is None:\n",
    "        state = states[player.next_state_id]\n",
    "        \n",
    "        spositions = lab.utils.quat_mul_vec(player.quaternions[0][np.newaxis,:], target_positions) + player.positions[0] - np.array([posx,0,posz])\n",
    "        target_state_id = np.sum(spositions*spositions, axis=1).argmin()\n",
    "        \n",
    "        action_id = value_function[player.next_state_id * target_positions_count + target_state_id, :].argmax()\n",
    "        if is_immediate:\n",
    "            action_id = immediate_rewards[player.next_state_id * target_positions_count + target_state_id, :].argmax()\n",
    "\n",
    "        player.selected_position = target_state_id\n",
    "        player.selected_qp = (player.quaternions[0], player.positions[0])\n",
    "        \n",
    "        action = state.actions[action_id]\n",
    "        player.set_action(action)\n",
    "                \n",
    "    player.tick()\n",
    "    display((player.last_played_frame, player.transition, is_immediate))\n",
    "    \n",
    "    q = player.quaternions\n",
    "    p = player.positions\n",
    " \n",
    "    a = lab.utils.quat_to_mat(q, p)\n",
    "    viewer.set_shadow_poi(p[0])\n",
    "    \n",
    "    viewer.begin_shadow()\n",
    "    viewer.draw(character, a)\n",
    "    viewer.end_shadow()\n",
    "    \n",
    "    viewer.begin_display()\n",
    "    viewer.draw_ground()\n",
    "    viewer.draw(character, a)\n",
    "    if is_immediate:\n",
    "        sphere.materials()[0].set_albedo(np.array([1,0,0], dtype=np.float32))\n",
    "    else:\n",
    "        sphere.materials()[0].set_albedo(np.array([0,1,1], dtype=np.float32))\n",
    "    target_matrix = np.eye(4, dtype=np.float32)\n",
    "    target_matrix[0, 3] = posx\n",
    "    target_matrix[2, 3] = posz\n",
    "    viewer.draw(sphere, target_matrix)\n",
    "    viewer.end_display()\n",
    "\n",
    "    viewer.disable(depth_test=True)\n",
    "\n",
    "    states_matrices = np.eye(4, dtype=np.float32)[np.newaxis,...].repeat(target_positions.shape[0], axis=0)\n",
    "    states_matrices[:, :3, 3] = lab.utils.quat_mul_vec(player.selected_qp[0][np.newaxis,:], target_positions) + player.selected_qp[1]\n",
    "    viewer.draw_axis(states_matrices, 2)  \n",
    "    \n",
    "    viewer.draw_axis(states_matrices[player.selected_position][np.newaxis, ...], 10)\n",
    "    if player.current_action:\n",
    "        lines = lab.utils.quat_mul_vec(player.selected_qp[0][np.newaxis,:], player.current_action.trajectory_pos) + player.selected_qp[1]\n",
    "        viewer.draw_lines(lines.repeat(2, axis=0)[1:-1].astype(np.float32))\n",
    "        \n",
    "    viewer.execute_commands()\n",
    "    \n",
    "interact(\n",
    "    render, \n",
    "    frame=lab.Timeline(max=100)\n",
    ")\n",
    "viewer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44fc43b-1f24-4e88-affe-4a7e5e693450",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(out_value_function)\n",
    "out_value_function.clear_output()\n",
    "\n",
    "future_reward_discount = .97\n",
    "value_function_static = np.zeros([state_count, max_action_count])\n",
    "\n",
    "last_total_reward = 0\n",
    "for epoch in range(1000):\n",
    "    for _ in range(30000):\n",
    "        state_id = randrange(0, state_count)\n",
    "        action_id = randrange(0, len(states[state_id].actions))\n",
    "        action = states[state_id].actions[action_id]\n",
    "\n",
    "        length = action.end - action.start\n",
    "\n",
    "        # reward\n",
    "        reward = 0\n",
    "        if action.start > 950 and action.start < 1200 and action.end > 950 and action.end < 1200 :\n",
    "            reward = 1\n",
    "\n",
    "        # future\n",
    "        next_state = action.next_state_id\n",
    "        next_max = np.max(value_function_static[next_state,:])\n",
    "\n",
    "        # Update\n",
    "        value_function_static[state_id, action_id] = reward + next_max * future_reward_discount**length\n",
    "\n",
    "\n",
    "    total_reward = np.sum(value_function_static)\n",
    "    with out_value_function:\n",
    "        out_value_function.clear_output()\n",
    "        display(f\"epoch {epoch} :: total {total_reward}, difference with previous epoch {total_reward - last_total_reward}\")\n",
    "    if np.abs(total_reward - last_total_reward) < .1:\n",
    "        last_total_reward = total_reward\n",
    "        break\n",
    "    last_total_reward = total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e74793-e862-458a-a5d3-458decdb8e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "player = AnimPlayer(0)\n",
    "\n",
    "player.selected_position = 0\n",
    "player.selected_qp = ()\n",
    "player.selected_mode = 0\n",
    "\n",
    "def render(frame):              \n",
    "\n",
    "    posx = -gamepad.axes[1].value * 500\n",
    "    posz = gamepad.axes[0].value * 500\n",
    "    \n",
    "    if player.current_action is None:\n",
    "        state = states[player.next_state_id]\n",
    "\n",
    "        spos = player.positions[0] - np.array([posx,0,posz])\n",
    "        if player.selected_mode == 0 and np.sum(spos*spos) < 2500:\n",
    "            player.selected_mode = 1\n",
    "        if player.selected_mode == 1 and np.sum(spos*spos) > 10000:\n",
    "            player.selected_mode = 0\n",
    "\n",
    "        action_id = 0\n",
    "        target_state_id = 0\n",
    "        if player.selected_mode == 0:\n",
    "            spositions = lab.utils.quat_mul_vec(player.quaternions[0][np.newaxis,:], target_positions) + player.positions[0] - np.array([posx,0,posz])\n",
    "            target_state_id = np.sum(spositions*spositions, axis=1).argmin()\n",
    "            \n",
    "            action_id = value_function[player.next_state_id * target_positions_count + target_state_id, :].argmax()\n",
    "        else:\n",
    "            action_id = value_function_static[player.next_state_id, :].argmax()\n",
    "\n",
    "        player.selected_position = target_state_id\n",
    "        player.selected_qp = (player.quaternions[0], player.positions[0])\n",
    "        \n",
    "        action = state.actions[action_id]\n",
    "        player.set_action(action)\n",
    "                \n",
    "    player.tick()\n",
    "    display((player.last_played_frame, player.transition, player.selected_mode))\n",
    "    \n",
    "    q = player.quaternions\n",
    "    p = player.positions\n",
    " \n",
    "    a = lab.utils.quat_to_mat(q, p)\n",
    "    viewer.set_shadow_poi(p[0])\n",
    "    \n",
    "    viewer.begin_shadow()\n",
    "    viewer.draw(character, a)\n",
    "    viewer.end_shadow()\n",
    "    \n",
    "    viewer.begin_display()\n",
    "    viewer.draw_ground()\n",
    "    viewer.draw(character, a)\n",
    "    if player.selected_mode:\n",
    "        sphere.materials()[0].set_albedo(np.array([0,1,0], dtype=np.float32))\n",
    "    else:\n",
    "        sphere.materials()[0].set_albedo(np.array([0,1,1], dtype=np.float32))\n",
    "    target_matrix = np.eye(4, dtype=np.float32)\n",
    "    target_matrix[0, 3] = posx\n",
    "    target_matrix[2, 3] = posz\n",
    "    viewer.draw(sphere, target_matrix)\n",
    "    viewer.end_display()\n",
    "\n",
    "    viewer.disable(depth_test=True)\n",
    "\n",
    "    if player.selected_mode == False:\n",
    "        states_matrices = np.eye(4, dtype=np.float32)[np.newaxis,...].repeat(target_positions.shape[0], axis=0)\n",
    "        states_matrices[:, :3, 3] = lab.utils.quat_mul_vec(player.selected_qp[0][np.newaxis,:], target_positions) + player.selected_qp[1]\n",
    "        viewer.draw_axis(states_matrices, 2)  \n",
    "        \n",
    "        viewer.draw_axis(states_matrices[player.selected_position][np.newaxis, ...], 10)\n",
    "        if player.current_action:\n",
    "            lines = lab.utils.quat_mul_vec(player.selected_qp[0][np.newaxis,:], player.current_action.trajectory_pos) + player.selected_qp[1]\n",
    "            viewer.draw_lines(lines.repeat(2, axis=0)[1:-1].astype(np.float32))\n",
    "        \n",
    "    viewer.execute_commands()\n",
    "    \n",
    "interact(\n",
    "    render, \n",
    "    frame=lab.Timeline(max=100)\n",
    ")\n",
    "viewer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
